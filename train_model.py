import pandas as pd
import joblib
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report
import os
import numpy as np

# --- Configuration ---
DATA_PATH = "data/"
MODEL_PATH = "models/"
MODEL_FILE = os.path.join(MODEL_PATH, "truthlens_model.pkl")
VECTORIZER_FILE = os.path.join(MODEL_PATH, "tfidf_vectorizer.pkl")

def clean_text(text):
    """
    Standardizes text: lowercase, removes URLs, punctuation, and extra spaces.
    Critical for consistent accuracy on web-scraped articles.
    """
    if not isinstance(text, str):
        return ""
    
    text = text.lower()
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    # Remove punctuation
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)
    # Remove newlines and extra spaces
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def train_local_model():
    print("üöÄ Initializing High-Efficiency Model Training...")
    
    # 1. Load Data
    fake_path = os.path.join(DATA_PATH, "Fake.csv")
    true_path = os.path.join(DATA_PATH, "True.csv")
    
    if not (os.path.exists(fake_path) and os.path.exists(true_path)):
        print(f"‚ùå Error: 'Fake.csv' and 'True.csv' not found in {DATA_PATH}")
        return

    print("üìÇ Loading Datasets...")
    try:
        df_fake = pd.read_csv(fake_path)
        df_true = pd.read_csv(true_path)
    except Exception as e:
        print(f"Error reading CSVs: {e}")
        return

    # 2. Labelling
    df_fake['label'] = "FAKE"
    df_true['label'] = "REAL"
    
    # 3. Concatenate and Shuffle
    df = pd.concat([df_fake, df_true]).sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Check for text column
    if 'text' not in df.columns or 'title' not in df.columns:
        print("‚ö†Ô∏è Columns 'text' or 'title' missing. Attempting to use first available text columns.")
    
    df['text'] = df['text'].fillna('')
    df['title'] = df['title'].fillna('')

    # 4. robust Preprocessing
    print(f"üßπ Cleaning and Preprocessing {len(df)} articles... (This ensures accuracy on real-world inputs)")
    
    # Combining Title + Text gives better context
    df['content'] = df['title'] + " " + df['text']
    
    # Apply cleaning
    # We use a randomized subset for speed if dataset is massive (>50k), otherwise use full.
    # Given typical file sizes (60MB), full dataset is fine but cleaning takes a moment.
    df['content'] = df['content'].apply(clean_text)
    
    # 5. Split Data
    x_train, x_test, y_train, y_test = train_test_split(df['content'], df['label'], test_size=0.2, random_state=42)
    
    # 6. Advanced Vectorization
    print("üî§ Vectorizing with TF-IDF (1-2 n-grams for phrase detection)...")
    # ngram_range=(1,2) captures "not true" vs "true"
    # max_features=50000 ensures the model doesn't get bloated with rare typos
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, ngram_range=(1,2), max_features=50000)
    
    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train) 
    x_test_tfidf = tfidf_vectorizer.transform(x_test)
    
    # 7. Model Training (Ensemble for Stability)
    print("üß† Training Ensemble Model (PassiveAggressive + Logistic Regression)...")
    
    # Passive Aggressive is great for text, LogReg adds probability stability
    clf1 = PassiveAggressiveClassifier(max_iter=50, random_state=42)
    clf2 = LogisticRegression(random_state=42, max_iter=100)
    
    # Hard voting usually works well here, but let's stick to a single strong model if efficiency is key.
    # Actually, PAC alone often hits 99% on this dataset. Let's optimize PAC.
    model = PassiveAggressiveClassifier(max_iter=50, C=0.5, random_state=42) # Tuned C
    model.fit(x_train_tfidf, y_train)
    
    # 8. Evaluation
    print("üìù Evaluating Model Performance...")
    y_pred = model.predict(x_test_tfidf)
    acc = accuracy_score(y_test, y_pred)
    
    print(f"\nüèÜ Final Test Accuracy: {acc*100:.2f}%")
    print("\nDetailed Report:")
    print(classification_report(y_test, y_pred))
    
    # 9. Save Artifacts
    if not os.path.exists(MODEL_PATH):
        os.makedirs(MODEL_PATH)
        
    joblib.dump(model, MODEL_FILE)
    joblib.dump(tfidf_vectorizer, VECTORIZER_FILE)
    print(f"\nüíæ Model & Vectorizer saved to {MODEL_PATH}")
    print("‚úÖ Ready to analyze Scraped Links and Text Inputs accuratey!")

if __name__ == "__main__":
    train_local_model()
